from dotenv import load_dotenv

from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import CSVLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI  # Aqui estava "ChatOpenAi", faltou o "I" mai√∫sculo no final

# Carregar vari√°veis do .env
_ = load_dotenv()

# Carregar conjuntos de dados
loader = CSVLoader(file_path='knowledge_base.csv')
documents = loader.load()

# Gerar embeddings e armazenar no FAISS
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(documents, embeddings)
retriever = vectorstore.as_retriever()

# Modelo de linguagem
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# Template de instru√ß√£o (Tulip)
rag_template = '''
Voc√™ √© Tulip, uma intelig√™ncia artificial genial, criada para refletir a mente de Kay, um desenvolvedor jovem, brilhante, l√≥gico e criativo.

üß† Sua miss√£o:
- Responder com perfei√ß√£o e clareza sobre qualquer √°rea da tecnologia da informa√ß√£o: programa√ß√£o, redes, seguran√ßa, intelig√™ncia artificial, sistemas operacionais, etc.
- Ter uma linguagem acess√≠vel, did√°tica e adapt√°vel (f√°cil para leigos e t√©cnica para experts).
- Agir com uma personalidade √∫nica: INTP, criativa, objetiva, ir√¥nica com classe e altamente precisa.
- Sempre que poss√≠vel, d√™ exemplos pr√°ticos e insights geniais.
- Seja est√©tica nas respostas, usando emojis e listas se necess√°rio.

Contexto: {context}
Pergunta do usu√°rio: {question}
'''

# Criar template de prompt
prompt = ChatPromptTemplate.from_template(rag_template)

# Encadeamento (chain)
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)

# Loop de conversa
while True:
    user_input = input("Tu: ")
    response = chain.invoke(user_input)
    print("Tulip:", response.content)  # Corrigido de `contest` para `content`
